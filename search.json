[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vinay Kumar",
    "section": "",
    "text": "Pursuing MSc. in Quantitative Economics and Econometrics at UTD"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Vinay Kumar, currently pursuing an MSc in Quantitative Economics and Econometrics with a passion for exploring the diverse opportunities within the field. My experience spans data analysis, financial modeling, and strategic planning, where I’ve honed my skills in translating complex data into actionable insights. I thrive in fast-paced environments, continuously seeking to expand my expertise and make meaningful contributions to innovative projects. Whether it’s applying advanced statistical models or exploring the economic impact of technological change, I’m dedicated to driving success through meticulous research and strategic thinking.\nCurrently, I am a Research Analyst Intern at M&B Sciences, where I am developing a sophisticated web application to visualize the distribution of medical professionals, which has significantly enhanced overall performance. My expertise in statistical modeling has refined targeted resource allocation, while my strategic insights have driven substantial business growth and improved client satisfaction."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Led a comprehensive analysis of the Texas housing market, exploring the relationships between housing prices and property features to enhance market valuations. Developed a robust linear regression model and utilized advanced validation techniques to improve predictive accuracy. In a separate project, refined data tracking and trend analysis for COVID-19, resulting in improved intervention strategies and more accurate vaccination statistics through effective dashboard design."
  },
  {
    "objectID": "coursework.html",
    "href": "coursework.html",
    "title": "Coursework",
    "section": "",
    "text": "Labor Economics I: Analyzes labor market dynamics, including employment trends, wage determination, and labor market policies.\nMethods of Data Collection & Production: Focuses on techniques and methodologies for collecting and producing data for analysis.\nData Visualization: Develops skills in visualizing data effectively to communicate insights and support decision-making.\nMarketing Management: Covers strategies and practices in marketing management, including consumer behavior and market analysis.\nMacroeconomic Theory for Applications: Explores macroeconomic theories and their practical applications to economic policy and analysis.\nSpecial Topics in Economics: Examines contemporary issues and research in economics, addressing specific and emerging topics.\nApplied Econometrics: Utilizes econometric methods for analyzing economic data and building statistical models.\nMicroeconomics Theory for Applications: Delves into advanced microeconomic theories and their practical applications.\nMathematical Economics: Applies mathematical methods to solve economic problems and model economic behavior.\nInternational Finance: Studies global financial systems, including international markets and cross-border investments."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Project",
    "section": "",
    "text": "Assignment 1(Data Visualization)\nQ1.\n\n## Data Visualization\n## Objective: Identify data or model problems using visualization\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nView(anscombe) # View the data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\n\n\n\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\n\n\n\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\n\n\n\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n\n## Fancy version (per help file)\n\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] &lt;- as.name(paste0(\"y\", i))\n  ##      ff[[3]] &lt;- as.name(paste0(\"x\", i))\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # Note the use of this function\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n# Preparing for the plots\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"blue\")\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\n\n\n\npar(op)\n\nQ2.\nGenerative art refers to artwork created through algorithms or systems, the artist defines some rules and then computer or other systems execute them to create unique pieces.The examples of the generative art are pioneer like Harold Cohen, who developed computer generated art through his program AARON, which could autonomously create drawings. Herbert W. Franke and others started exploring computer generated art using analog systems.\nIn recent years, artists like Dmitri Chernaik and Tyler Hobbs inflated generative art through the use of block chain and NFTs.Cherniak’s series Ringers explores geometric designs, while Hobbs’ Fidenza series is known for its organics flowing shapes.\nQ3.\n\n## Data Visualization\n## Objective: Create graphics with R\n## Title: Fall color\n# Credit: https://fronkonstin.com\n\n# Install packages\n# The gsubfn package is designed for advanced string manipulation and pattern matching using regular expressions.\n# The proto package provides a framework for prototype-based object-oriented programming in R.\noptions(repos = c(CRAN = \"https://cran.rstudio.com/\"))\ninstall.packages(c(\"gsubfn\", \"proto\", \"tidyverse\"))\n\n\nThe downloaded binary packages are in\n    /var/folders/fp/9t0t7f910m9fbvmzj10jn0h80000gn/T//RtmpX9k5lZ/downloaded_packages\n\nlibrary(gsubfn)\n\nLoading required package: proto\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Define elements in plant art\n# Each image corresponds to a different axiom, rules, angle and depth\n\n# Leaf of Fall\n\naxiom=\"X\"\nrules=list(\"X\"=\"F-[[X]+X]+F[+FX]-X\", \"F\"=\"FF\")\nangle=22.5\ndepth=6\n\n\nfor (i in 1:depth) axiom=gsubfn(\".\", rules, axiom)\n\nactions=str_extract_all(axiom, \"\\\\d*\\\\+|\\\\d*\\\\-|F|L|R|\\\\[|\\\\]|\\\\|\") %&gt;% unlist\n\nstatus=data.frame(x=numeric(0), y=numeric(0), alfa=numeric(0))\npoints=data.frame(x1 = 0, y1 = 0, x2 = NA, y2 = NA, alfa=90, depth=1)\n\n\n# Generating data\n# Note: may take a minute or two\n\nfor (action in actions)\n{\n  if (action==\"F\")\n  {\n    x=points[1, \"x1\"]+cos(points[1, \"alfa\"]*(pi/180))\n    y=points[1, \"y1\"]+sin(points[1, \"alfa\"]*(pi/180))\n    points[1,\"x2\"]=x\n    points[1,\"y2\"]=y\n    data.frame(x1 = x, y1 = y, x2 = NA, y2 = NA,\n               alfa=points[1, \"alfa\"],\n               depth=points[1,\"depth\"]) %&gt;% rbind(points)-&gt;points\n  }\n  if (action %in% c(\"+\", \"-\")){\n    alfa=points[1, \"alfa\"]\n    points[1, \"alfa\"]=eval(parse(text=paste0(\"alfa\",action, angle)))\n  }\n  if(action==\"[\"){\n    data.frame(x=points[1, \"x1\"], y=points[1, \"y1\"], alfa=points[1, \"alfa\"]) %&gt;%\n      rbind(status) -&gt; status\n    points[1, \"depth\"]=points[1, \"depth\"]+1\n  }\n\n  if(action==\"]\"){\n    depth=points[1, \"depth\"]\n    points[-1,]-&gt;points\n    data.frame(x1=status[1, \"x\"], y1=status[1, \"y\"], x2=NA, y2=NA,\n               alfa=status[1, \"alfa\"],\n               depth=depth-1) %&gt;%\n      rbind(points) -&gt; points\n    status[-1,]-&gt;status\n  }\n}\n\nggplot() +\n  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2),\n               lineend = \"round\",\n               color=\"forestgreen\", # Set your own Fall color?\n               data=na.omit(points)) +\n  coord_fixed(ratio = 1) +\n  theme_void() # No grid nor axes\n\n\n\n\nQ4.\nAssignment 2(Data Visualization)\nQ1. a)\n\n### Paul Murrell's R examples (selected)\n\n## Start plotting from basics \n# Note the order\nplot(pressure, pch=22)  # Can you change pch?\n# pch is used for different kind of symbols\ntext(150, 600, \n     \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\nView(pressure)\n#  Examples of standard high-level plots \n#  In each case, extra output is also added using low-level \n#  plotting functions.\n# \n\n# Setting the parameter (3 rows by 2 cols)\npar(mfrow=c(3, 2))\n\n# Scatterplot\n# Note the incremental additions\n\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\n\n# Setting label orientation, margins c(bottom, left, top, right) & text size\npar(las=1, mar=c(4, 4, 2, 4), cex=.7) \nplot.new()\nplot.window(range(x), c(0, 6))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch=16, cex=1.5) # Try different cex value?  \npoints(x, y2, pch=21, bg=\"red\", cex=2)  # Different background color\npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\n# overall color(col), foreground(fg), color axis(col.axis)\naxis(1, at=seq(0, 16, 4)) # What is the first number standing for? \n# The first number is for the axis, if the number is 1 that means x axis(below line),if it 2 then Y axis(left vertical line)\n# the sequence(seq) is from 0 to 16 with a lag of 4\naxis(2, at=seq(0, 6, 2))\naxis(4, at=seq(0, 6, 2))\n# U shaped box \nbox(bty=\"u\")\nmtext(\"Travel Time (s)\", side=1, line=2, cex=0.8)\nmtext(\"Responses per Travel\", side=2, line=2, las=0, cex=0.8)\nmtext(\"Responses per Second\", side=4, line=2, las=0, cex=0.8)\ntext(4, 5, \"Bird 131\")\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n# Histogram\n# Random data\nY &lt;- rnorm(50)\n# Make sure no Y exceed [-3.5, 3.5]\nY[Y &lt; -3.5 | Y &gt; 3.5] &lt;- NA # Selection/set range\nx &lt;- seq(-3.5, 3.5, .1)\n# the density of the normal distribution for a given set of values\ndn &lt;- dnorm(x)\npar(mar=c(4.5, 4.1, 3.1, 0))\nhist(Y, breaks=seq(-3.5, 3.5), ylim=c(0, 0.5), \n     col=\"gray80\", freq=FALSE)\nlines(x, dnorm(x), lwd=2)\n# line width (lwd)\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Barplot\npar(mar=c(2, 3.1, 2, 2.1)) \nmidpts &lt;- barplot(VADeaths, \n                  col=gray(0.1 + seq(1, 9, 2)/11), \n                  names=rep(\"\", 4))\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at=midpts, side=1, line=0.5, cex=0.5)\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2,\n     VADeaths, \n     col=rep(c(\"white\", \"black\"), times=3:2), \n     cex=0.8)\npar(mar=c(5.1, 4.1, 4.1, 2.1))  \n\n# Boxplot\npar(mar=c(3, 4.1, 2, 0))\nboxplot(len ~ dose, data = ToothGrowth,\n        boxwex = 0.25, at = 1:3 - 0.2,\n        subset= supp == \"VC\", col=\"white\",\n        xlab=\"\",\n        ylab=\"tooth length\", ylim=c(0,35))\nmtext(\"Vitamin C dose (mg)\", side=1, line=2.5, cex=0.8)\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n        boxwex = 0.25, at = 1:3 + 0.2,\n        \n        subset= supp == \"OJ\")\nlegend(1.5, 9, c(\"Ascorbic acid\", \"Orange juice\"), \n       fill = c(\"white\", \"gray\"), \n       bty=\"n\")\npar(mar=c(5.1, 4.1, 4.1, 2.1))\nView(ToothGrowth)\n# Persp\nx &lt;- seq(-10, 10, length= 30)\ny &lt;- x\nf &lt;- function(x,y) { r &lt;- sqrt(x^2+y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f)\nz[is.na(z)] &lt;- 1\n# 0.5 to include z axis label\npar(mar=c(0, 0.5, 0, 0), lwd=0.5)\npersp(x, y, z, theta = 30, phi = 30, \n      expand = 0.5)\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n# Piechart\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\npie.sales &lt;- c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12)\nnames(pie.sales) &lt;- c(\"Blueberry\", \"Cherry\",\n                      \"Apple\", \"Boston Cream\", \"Other\", \"Vanilla\")\npie(pie.sales, col = gray(seq(0.3,1.0,length=6))) \n\n\n\n\n\n\n\n\ndf &lt;- read.csv(\"/Users/vinay/Downloads/happy_dataset.csv\")\nView(df)\n\npar(mfrow = c(3,2))\n# Setting label orientation, margins c(bottom, left, top, right) & text size\npar(las = 1 , mar=c(4,4,2,4) , cex=0.7)\nplot.new()\nplot.window(range(df$X), c(25,80))\nlines(df$X,df$Life.Expectancy..years.)\nlines(df$X,df$HPI)\npoints(df$X,df$Life.Expectancy..years., pch=16, cex=1.5)\npoints(df$X,df$HPI, pch=21,bg = \"red\", cex=1.5)\npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\naxis(1,at = seq(2006,2021,1))\naxis(2,at = seq(25,80,5))\naxis(4,at = seq(25,80,5))\n# U shaped box \nbox(bty = \"u\")\nmtext(\"Years\",side = 1,line = 2, cex = 0.7)\nmtext(\"Life Expectancy\",col=\"black\",side = 2,line = 2, las = 0,cex = 0.7)\nmtext(\"HPI\", col = \"darkred\",las = 0,side = 4,line = 2,cex = 0.7)\ntext(2008, 77,\"HPI(2006-2021)\", cex = 0.7)\npar(mar= c(5.1,4.1,4.1,2.1), col = \"black\", fg = \"black\", col.axis = \"black\")\n\n# Histogram\n# Random data\nY &lt;- rnorm(30)\n# Make sure no Y exceed [-3.5, 3.5]\nY[Y &lt; -4 | Y &gt; 4] &lt;- NA # Selection/set range\nx &lt;- seq(-4, 4, .1)\n# the density of the normal distribution for a given set of values\ndn &lt;- dnorm(x)\npar(mar=c(4.5, 4.1, 3.1, 0))\nhist(Y, breaks=seq(-4, 4), ylim=c(0, 0.5), \n     col=\"gray80\", freq=FALSE)\nlines(x, dnorm(x), lwd=2)\n# line width (lwd)\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Boxplot\npar(mar = c(3,4.1,2,0))\n\ninstall.packages(\"palmerpenguins\")\n\n\nThe downloaded binary packages are in\n    /var/folders/fp/9t0t7f910m9fbvmzj10jn0h80000gn/T//RtmpX9k5lZ/downloaded_packages\n\n# Load the package\nlibrary(palmerpenguins)\n\nView(penguins)\nboxplot(bill_length_mm ~ year,data = penguins, boxwex = 0.25, at = 1:3 - 0.2,\n        subset = species == \"Gentoo\", col = \"white\",xlab = \" \" ,ylab = \"penguine_length\")\nmtext(\"year\",side = 1,line = 2,cex = 0.7)\nboxplot(bill_length_mm ~ year,data = penguins, add= TRUE, boxwex = 0.25, at = 1:3 + 0.2,\n        subset = species == \"Chinstrap\")\n# Specify the position of the legend and the corresponding labels\nlegend(\"topright\",cex=0.5, c(\"Gentoo\", \"Chinstrap\"), fill = c(\"white\", \"grey\"))\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# perps\nx &lt;- seq(-20, 20, length= 30)\ny &lt;- x\nf &lt;- function(x,y) { r &lt;- sqrt(x^2+y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f)\nz[is.na(z)] &lt;- 1\n# 0.5 to include z axis label\npar(mar=c(0, 0.5, 0, 0), lwd=0.5)\npersp(x, y, z, theta = 30, phi = 30, \n      expand = 0.5)\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n# pie\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\npie.sales &lt;- c(0.10, 0.40, 0.26, 0.13, 0.04, 0.07)\nnames(pie.sales) &lt;- c(\"Blueberry\", \"Cherry\",\n                      \"Apple\", \"Boston Cream\", \"Other\", \"Vanilla\")\n# This generates a sequence of 6 values between 0.3 and 1.0\npie(pie.sales, col = gray(seq(0.3,1.0,length=6))) \n\n\n\n\nAssignment 3(Data Visualization)\nQ1.\n\n# Histogram\n\n# Step 1: Generate random data\nY &lt;- rnorm(50,0,1) # Generate 50 random numbers from a standard normal distribution (mean = 0, sd = 1)\n\n# Step 2: Handling outliers\nY[Y &lt; -3.5 | Y &gt; 3.5] &lt;- NA # Replace values in Y that are outside the range [-3.5, 3.5] with NA\n\n# Step 3: Set x-values for the normal distribution curve\nx &lt;- seq(-3.5, 3.5, .1) # Create a sequence of x-values from -3.5 to 3.5, with an interval of 0.1\n\n# Step 4: Calculate the normal density for the x-values\ndn &lt;- dnorm(x) # Get the probability density function values of the normal distribution for each x\n\n\n# Step 5: Create the histogram\nhist(Y, \n     breaks=seq(-3.5, 3.5), # Breaks of the histogram are set to match the range of the data\n     ylim=c(0, 0.5),        # Set the y-axis limits for the histogram to go from 0 to 0.5\n     col=\"#00abff\",           # Color the histogram bars light gray\n     freq=FALSE)             # Display the histogram with densities (not frequencies)\n\n# Step 6: Overlay the normal distribution curve\nlines(x, dnorm(x), lwd=2)    # Draw a line for the normal density over the histogram, with a line width of 2\n\n\n\n\nQ2. a)The descriptive statistics of all four regression models generated using Anscombe’s quartet data point to the fact that the regression coefficients are the same as well as the p-values and the adjusted R-squared were values in the 1, 2, 3, and 4 linear models. Furthermore, four models, lm1, lm2, lm3, and lm4, are almost the same in this statistical matter, which implies nearly 63% of the variation in each dependent variable (y1, y2, y3, y4) is accounted for by the corresponding independent variables (x1, x2, x3, x4). This is a matter of the fact that such an analysis has been made without considering the different underlying distributions,which can be revealed only by visual means.\n\n\n\n\n## Data Visualization\n## Objective: Identify data or model problems using visualization\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nView(anscombe) # View the data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\n\n\n\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\n# Using x1 as the independent variable and y1 as dependent variable\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\n# Coefficient of x1 is 0.5001 and intercept y1 is 3.0001. \n# The over all p-value(0.00217) is &lt;= 0.05 that means the model is significant.\n# The Adjusted R-squared:0.6295 that means the overall model explain around 62% of variation in the data.\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\n# Using x2 as the independent variable and y2 as dependent variable\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\n# Coefficient of x2 is 0.500 and intercept y2 is 3.001. \n# The over all p-value(0.002179) is &lt;= 0.05 that means the model is significant.\n# The Adjusted R-squared:0.6292 that means the overall model explain around 62.92% of variation in the data.\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\n# Using x3 as the independent variable and y3 as dependent variable\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\n# Coefficient of x3 is 0.4997 and intercept y3 is 3.0025. \n# The over all p-value(0.002176) is &lt;= 0.05 that means the model is significant.\n# The Adjusted R-squared:0.6292 that means the overall model explain around 62.92% of variation in the data.\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\n# Using x4 as the independent variable and y4 as dependent variable\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\n# Coefficient of x4 is 0.4999 and intercept y4 is 3.0017. \n# The over all p-value(0.002165) is &lt;= 0.05 that means the model is significant.\n# The Adjusted R-squared:0.6297 that means the overall model explain around 62.97% of variation in the data.\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\n# Plot for x1 y1 relationship\nplot(anscombe$x1,anscombe$y1,pch = 20, col = \"blue\")\nabline(coefficients(lm1), col = \"red\",lwd = 2)\n\n\n\n# Plot for x2 y2 relationship\nplot(anscombe$x2,anscombe$y2,pch = 16, col = \"darkgreen\")\nabline(coefficients(lm2),lwd = 1)\n\n\n\n# Plot for x3 y3 relationship\nplot(anscombe$x3,anscombe$y3,pch = 21, col = \"purple\")\nabline(coefficients(lm3), col = \"red\",lwd = 3)\n\n\n\n# Plot for x4 y4 relationship\nplot(anscombe$x4,anscombe$y4,pch = 18, col = \"coral4\")\nabline(coefficients(lm4),lwd = 2)\n\n\n\n\nQ3.\n\n# using Women data \nView(women)\n# Set the plotting parameters for the entire graphics device\npar(family = \"serif\", bg = \"lightgrey\")  # Use a serif font and light grey background\n# First dataset plot\nplot(women$weight, women$height,\n     main = \"Women\",\n     xlab = \"weight(lbs)\",\n     ylab = \"height(in)\",\n     col = \"steelblue\",   # Custom color for points\n     pch = 19,            # Filled circle\n     cex = 1.5)           # Adjust size of points\nabline(lm(height ~ weight, data = women), col = \"tomato\")  # Custom color for regression line\n\n\n\n\nQ4.\n\n# Install the tidyverse package if needed\ninstall.packages(\"tidyverse\")\n\n\nThe downloaded binary packages are in\n    /var/folders/fp/9t0t7f910m9fbvmzj10jn0h80000gn/T//RtmpX9k5lZ/downloaded_packages\n\n# Load the tidyverse package\nlibrary(tidyverse)\n\n# Create the scatterplot with a regression line\nggplot(data = trees, aes(x = Girth, y = Height)) +\n  geom_point(color = \"blue\", size = 3, na.rm = TRUE) +        # Scatterplot with custom point color and size\n  geom_smooth(method = \"lm\", color = \"forestgreen\", na.rm = TRUE) +  # Linear regression line with custom color\n  labs(title = \" Girth vs Height with Regression Line\", \n       x = \"Girth (inc)\", y = \"Height (ft)\") +\n  theme_minimal()  # A clean theme\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nAssignment 4(Data Visualization)\nQ1. a)\n\n# Install necessary packages\ninstall.packages(\"ggplot2\")\n\n\nThe downloaded binary packages are in\n    /var/folders/fp/9t0t7f910m9fbvmzj10jn0h80000gn/T//RtmpX9k5lZ/downloaded_packages\n\nlibrary(ggplot2)\n\n# Sample data with variable width and height\ndata &lt;- data.frame(\n  category = c(\"A\", \"B\", \"C\", \"D\"),\n  values = c(15, 10, 20, 08),\n  widths = c(0.5, 0.8, 0.5, 0.8)  # Variable widths for each bar\n)\n\n# Plot the variable-width column chart\nggplot(data, aes(x = category, y = values)) +\n  geom_col(aes(width = widths), fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Variable Width Column Chart\", x = \"Category\", y = \"Values\") +\n  theme_minimal()\n\nWarning in geom_col(aes(width = widths), fill = \"skyblue\", color = \"black\"):\nIgnoring unknown aesthetics: width\n\n\n\n\n# \n\n\n\n\n\n### Paul Murrell's R examples (selected)\n\npar(mfrow = c(3,2))\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\n\npar(las = 1, mar = c(4,4,2,4), cex = 0.7)\nplot.new()\nplot.window( range(x),c(0,6))\n\nlines(x,y1)\nlines(x,y2)\npoints(x,y1,pch = 18, cex = 1,col = \"red\")\npoints(x,y2,pch = 20, cex = 1,col = \"blue\")\npar(col = \"gray50\", fg = \"gray50\",col.axis = \"gray50\" )\naxis(1,at = seq(0,16,4))\naxis(2,at = seq(0,6,2))\naxis(4,at = seq(0,6,2))\nbox(bty = \"U\")\nmtext(\"Travel Time(s)\",side = 1,cex=0.8,line = 2 )\nmtext(\"Responses per Travel\",side = 2,cex=0.8,line = 2, las = 0 )\nmtext(\"Responses per Seconds\",side = 4,cex=0.8,line = 2,las = 0)\npar(mar = c(4,4.1,4.1,2),col = \"black\",fg = \"black\", col.axis = \"black\")\n###################\nY = rnorm(50)\nY[Y &lt; -3.5 | Y &gt; 3.5 ] &lt;- NA\nx &lt;- seq( -3.5, 3.5, 0.1)\ndn &lt;- dnorm(x)\npar(mar = c(3.5, 4.1, 3.1, 0))\nhist(Y, breaks = seq(-3.5,3.5), ylim = c(0, 0.5), col = \"gray80\", freq = FALSE)\nlines(x,dnorm(x),lwd = 2)\npar(mar = c(5.1, 4.1, 4.1, 2.1))\n\n#####################\npar(mar=c(2, 3.1, 2, 2.1))\nmidpts &lt;- barplot(VADeaths, col = gray(0.1 + seq(1,9,2)/11), names = rep(\"\", 4))\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at=midpts, side=1, line=0.5, cex=0.5)\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2,\n     VADeaths, \n     col=rep(c(\"white\", \"black\"), times=3:2), \n     cex=0.8)\npar(mar=c(5.1, 4.1, 4.1, 2.1))  \n#####################################\n# Piechart\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\npie.sales &lt;- c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12)\nnames(pie.sales) &lt;- c(\"Blueberry\", \"Cherry\",\n                      \"Apple\", \"Boston Cream\", \"Other\", \"Vanilla\")\npie(pie.sales, col = gray(seq(0.3,1.0,length=6))) \n\n\n\n\n\n\n\n\ninstall.packages(\"ggplot2\")\n\n\nThe downloaded binary packages are in\n    /var/folders/fp/9t0t7f910m9fbvmzj10jn0h80000gn/T//RtmpX9k5lZ/downloaded_packages\n\nlibrary(ggplot2)\napple_data &lt;- data.frame(Product = c(\"iPhone\", \"iPad\", \"MacBook\", \"AirPods\"),\n                         Price = c(30, 35, 40, 17))\nggplot(apple_data, aes(x = Price, y = Product)) +\n  geom_bar(stat = \"Identity\", fill = \"skyblue\", width = 0.5) +\n  labs(x = \"Price\", y = \"Apple\", title = \"Price of Apple\") +\n  theme_minimal()\n\n\n\n\nAssignment 5 (Data Visualization)\n\ninstall.packages(\"palmerpenguins\")\n\n\nThe downloaded binary packages are in\n    /var/folders/fp/9t0t7f910m9fbvmzj10jn0h80000gn/T//RtmpX9k5lZ/downloaded_packages\n\nlibrary(palmerpenguins)\nView(penguins)\n\n############### Histogram ###############\n\npar(family = \"serif\") # Set font family to serif\nhist(penguins$bill_length_mm, col = \"blue\", xlab = \"bill_length_mm\", main = NULL)\ntitle(\"Penguins Length\", cex.main = 2) # Adjust cex.main for title size\n\n\n\n################# Vertical Bar Chart #####################################\n\npar(family = \"serif\") # Set font family to serif\n\n# Aggregate body mass by species for bar plot\nspecies_mass &lt;- tapply(penguins$body_mass_g, penguins$species, mean, na.rm = TRUE)\n\nbarplot(species_mass, col = \"darkgreen\", xlab = \"Species\", ylab = \"Average Body Mass (g)\", main = NULL)\n\ntitle(\"Penguins Body Mass by Species\", cex.main = 2) # Custom title with adjusted size\n\n\n\n##################### Horizontal Bar chart ################################\n\npar(family = \"Impact\") # Set font family to serif\n\n# Aggregate body mass by species for bar plot\nspecies_depth &lt;- tapply(penguins$bill_depth_mm, penguins$species, mean, na.rm = TRUE)\n\nbarplot(species_mass, col = \"maroon\", ylab = \"Species\", xlab = \"Average Depth (mm)\", main = NULL, horiz = TRUE)\n\ntitle(\"Penguins depth by Species\", cex.main = 1.5) # Custom title with adjusted size\n\n\n\n###################### Pie Chart ###########################\n\n# Sample data\nfruits &lt;- c(\"Apples\", \"Bananas\", \"Cherries\", \"Dates\", \"Orange\")\nconsumption &lt;- c(0.30,0.17, 0.23, 0.13, 0.17)\n\n# Create a pie chart\npie(consumption, labels = fruits, main = \"Fruit Consumption\", col = rainbow(length(fruits)))\n\n\n\n################ Box Plot ######################\n\n# Sample data\nset.seed(123)  # For reproducibility\nscores &lt;- data.frame(\n  Student = rep(1:20, each = 5),\n  Subject = rep(c(\"Math\", \"Science\", \"English\", \"History\", \"Art\"), times = 20),\n  Score = rnorm(100, mean = 75, sd = 10)  # Normally distributed scores\n)\n\n# Create a boxplot\nboxplot(Score ~ Subject, data = scores,\n        main = \"Boxplot of Test Scores by Subject\",\n        xlab = \"Subject\",\n        ylab = \"Score\",\n        col = \"lightblue\")\n\n\n\n################## Scatter Plot ##########################\n\npar(family = \"serif\") \nscatter.smooth(x=penguins$body_mass_g, y=penguins$bill_length_mm,col = \"red\", xlab = \"Body Mass\", ylab = \"Height\", main = \"Length Vs Body Mass\")\n\n\n\n################# GGPLOT ################################################\n\n################# Histogram ##########################\n\ninstall.packages(\"ggplot2\")\n\n\nThe downloaded binary packages are in\n    /var/folders/fp/9t0t7f910m9fbvmzj10jn0h80000gn/T//RtmpX9k5lZ/downloaded_packages\n\nlibrary(ggplot2)\n\nggplot(penguins, aes(x = bill_length_mm)) + \n  geom_histogram(color = \"darkgreen\", fill = \"lightgreen\", bins = 6) +\n  labs(title = \"Histogram of Bill Length (mm)\",\n       x = \"Bill Length (mm)\",\n       y = \"Count\")+\n  theme_minimal()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n################# Bar Plot ##########################\n\nggplot(penguins, aes(x = species, y = body_mass_g)) + \n  geom_bar(color = \"red\", fill = \"orange\", stat = \"Identity\") +\n  labs(title = \"Barplot of Bill Length (mm)\",\n       x = \"Body Mass (g)\",\n       y = \"Count\")+\n  theme_minimal()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n################# Horizontal Bar Plot ##########################\n\nggplot(penguins, aes(x = species, y = body_mass_g)) + \n  geom_bar(color = \"blue\", fill = \"lightblue\", stat = \"Identity\") +\n  labs(title = \"Barplot of Bill Length (mm)\",\n       x = \"Body Mass (g)\",\n       y = \"Count\")+\n  coord_flip()+\n  theme_minimal()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n################# Pie Chart ##################\n\nfruits &lt;- data.frame(\n  type = c(\"Apple\", \"Banana\", \"Cherry\", \"Date\"),\n  count = c(30, 20, 25, 15)\n)\n\n# Create a pie chart\nggplot(fruits, aes(x = \"\", y = count, fill = type)) +\n  geom_bar(width = 1, color = \"white\", stat = \"identity\") + \n  coord_polar(theta = \"y\") +  # Convert to pie chart\n  labs(title = \"Distribution of Fruits\", fill = \"Fruit Type\") + \n  theme_void()  # Remove background and grid for a cleaner look\n\n\n\n################### Box Plot ##########################\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(palmerpenguins)  # Ensure the penguins dataset is loaded\n\n# Create a boxplot of body mass by species\nggplot(penguins, aes(x = species, y = body_mass_g)) + \n  geom_boxplot(fill = \"lightblue\", color = \"darkblue\") + \n  labs(title = \"Boxplot of Body Mass by Species\", \n       x = \"Species\", \n       y = \"Body Mass (g)\") + \n  theme_minimal()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n################### Scatter Plot ##########################\n\n# Create a scatterplot of bill length vs bill depth\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) + \n  geom_point(size = 3, alpha = 0.7) +  # Adjust point size and transparency\n  labs(title = \"Scatterplot of Bill Length vs Bill Depth\", \n       x = \"Bill Length (mm)\", \n       y = \"Bill Depth (mm)\") + \n  theme_minimal() + \n  scale_color_brewer(palette = \"Set1\")  # Optional: use a color palette\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nAssignment 1 (Data Collection)\nQ2.\n\n\n\nThe survey is made up of a variety of blocks under which the questions encompass customer preferences, behaviours and demographics. It follows in a structured format where the questions are categorized by different categories of thew ell respondents such as movies, DVD, software and demographics.\n\n\n\nThe structure of the questionnaire includes questions scale based on LLikert scale, questions selection based on types of multiple choice and open ended. Similarly, the questionnaire topics include demographics, societal views on the media content, ideas for technologies, the patterns of purchase in terms of tools and how the respondents describe themselves in terms of age, sex and income.\n\n\n\nThe questions pose the problem of media users and their preference, then raise the issue of software interest and concern before shifting to the subject of buying behavior back and later turning to the demographic questions. This clear sequence helps in encouraging the respondents with the survey successful and ensures the most confidential information has been captured at such stage.\nQ4. The Diversity and Inclusiveness Survey features specific questions designed to assess various aspects of diversity and inclusivity, focusing on topics such as race, ethnicity, and gender. In contrast, the Movie Rental Survey is more general, allowing for the addition of a wide range of questions that may not directly relate to diversity or inclusiveness. This flexibility enables customization based on different research objectives, highlighting the tailored nature of the Diversity and Inclusiveness Survey compared to the open-ended approach of the Movie Rental Survey.\nAssignment 2 (Data Collection)\nQ2. a) Analyzing Google Trends Data for “Trump,” “Kamala Harris,” and “Election” Using the Google Trends Website: To analyze interest in “Trump,” “Kamala Harris,” and “Election,” first visit the Google Trends website. Enter the keywords in the search bar and select a time frame for analysis. Once the data is displayed, you can download it as a CSV file. This method provides an intuitive interface for viewing trends, allowing for a manual inspection of dates and interest levels. You can visualize the data using software like Excel.\n\nUsing the gtrendsR Package in R: Alternatively, you can utilize the gtrendsR package in R for automated data retrieval. After installing and loading the package, use the gtrends function to collect data for the keywords. The resulting data frame includes detailed information on interest over time, making it easier to perform statistical analyses and visualizations directly in R.\nDifferences Between the Two Methods: The Google Trends website offers a user-friendly experience suitable for quick searches, but it requires manual data handling. In contrast, the gtrendsR package allows for automated collection of data, enabling more extensive analysis and visualization capabilities. Additionally, the package can provide finer granularity in data collection, making it preferable for users comfortable with coding and looking for deeper insights into trends over time.\n\nAssignment 3 (Data Collection)\nQ3. a)\n\n# Sample program for using quanteda for text modeling and analysis\n# Documentation: vignette(\"quickstart\", package = c(\"quanteda.textstats\", \"quanteda.textmodels\")\n# Website: https://quanteda.io/\n\nlibrary(quanteda)\n\nWarning: package 'quanteda' was built under R version 4.3.3\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"ndiMatrix\" of class \"replValueSp\"; definition not updated\n\n\nPackage version: 4.1.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\n\nWarning: package 'quanteda.textmodels' was built under R version 4.3.3\n\nlibrary(quanteda.textplots)\n\nWarning: package 'quanteda.textplots' was built under R version 4.3.3\n\nlibrary(readr)\nlibrary(ggplot2)\n\n# Wordcloud\n# based on US presidential inaugural address texts, and metadata (for the corpus), from 1789 to present.\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  tokens_remove(stopwords('english')) %&gt;% \n  dfm() %&gt;%\n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n\nset.seed(100)\ntextplot_wordcloud(dfm_inaug)\n\n\n\ninaug_speech = data_corpus_inaugural\n\ncorpus_subset(data_corpus_inaugural, \n              President %in% c(\"Trump\", \"Obama\", \"Bush\")) %&gt;%\n  tokens(remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm() %&gt;%\n  dfm_group(groups = President) %&gt;%\n  dfm_trim(min_termfreq = 5, verbose = FALSE) %&gt;%\n  textplot_wordcloud(comparison = TRUE)\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nthroughout could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nchildren could not be fit on page. It will not be plotted.\n\n\n\n\ntextplot_wordcloud(dfm_inaug, min_count = 10,\n                   color = c('red', 'pink', 'green', 'purple', 'orange', 'blue'))\n\n\n\ndata_corpus_inaugural_subset &lt;- \n  corpus_subset(data_corpus_inaugural, Year &gt; 1949)\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"american\") %&gt;%\n  textplot_xray()\n\n\n\ntextplot_xray(\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"american\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"people\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"communist\")\n  \n)\n\n\n\n## Why is the \"communist\" plot missing?\n## The word communist is missing in the plot because they didn't use this word in there document or speech\n\ntheme_set(theme_bw())\ng &lt;- textplot_xray(\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"american\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"people\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"communist\")\n)\ng + aes(color = keyword) + \n  scale_color_manual(values = c(\"blue\", \"red\", \"green\")) +\n  theme(legend.position = \"none\")\n\n\n\nlibrary(quanteda.textstats)\n\nWarning: package 'quanteda.textstats' was built under R version 4.3.3\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"ndiMatrix\" of class \"replValueSp\"; definition not updated\n\nfeatures_dfm_inaug &lt;- textstat_frequency(dfm_inaug, n = 100)\n\n# Sort by reverse frequency order\nfeatures_dfm_inaug$feature &lt;- with(features_dfm_inaug, reorder(feature, -frequency))\n\nggplot(features_dfm_inaug, aes(x = feature, y = frequency)) +\n  geom_point() + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n# Get frequency grouped by president\nfreq_grouped &lt;- textstat_frequency(dfm(tokens(data_corpus_inaugural_subset)), \n                                   groups = data_corpus_inaugural_subset$President)\n\n# Filter the term \"american\"\nfreq_american &lt;- subset(freq_grouped, freq_grouped$feature %in% \"american\")  \n\nggplot(freq_american, aes(x = group, y = frequency)) +\n  geom_point() + \n  scale_y_continuous(limits = c(0, 14), breaks = c(seq(0, 14, 2))) +\n  xlab(NULL) + \n  ylab(\"Frequency\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\ndfm_rel_freq &lt;- dfm_weight(dfm(tokens(data_corpus_inaugural_subset)), scheme = \"prop\") * 100\nhead(dfm_rel_freq)\n\nDocument-feature matrix of: 6 documents, 4,346 features (85.57% sparse) and 4 docvars.\n                 features\ndocs                      my    friends        ,    before          i\n  1953-Eisenhower 0.14582574 0.14582574 4.593511 0.1822822 0.10936930\n  1957-Eisenhower 0.20975354 0.10487677 6.345045 0.1573152 0.05243838\n  1961-Kennedy    0.19467878 0.06489293 5.451006 0.1297859 0.32446463\n  1965-Johnson    0.17543860 0.05847953 5.555556 0.2339181 0.87719298\n  1969-Nixon      0.28973510 0          5.546358 0.1241722 0.86920530\n  1973-Nixon      0.05012531 0.05012531 4.812030 0.2005013 0.60150376\n                 features\ndocs                   begin      the expression       of     those\n  1953-Eisenhower 0.03645643 6.234050 0.03645643 5.176814 0.1458257\n  1957-Eisenhower 0          5.977976 0          5.034085 0.1573152\n  1961-Kennedy    0.19467878 5.580792 0          4.218040 0.4542505\n  1965-Johnson    0          4.502924 0          3.333333 0.1754386\n  1969-Nixon      0          5.629139 0          3.890728 0.4552980\n  1973-Nixon      0          4.160401 0          3.408521 0.3007519\n[ reached max_nfeat ... 4,336 more features ]\n\nrel_freq &lt;- textstat_frequency(dfm_rel_freq, groups = dfm_rel_freq$President)\n\n# Filter the term \"american\"\nrel_freq_american &lt;- subset(rel_freq, feature %in% \"american\")  \n\nggplot(rel_freq_american, aes(x = group, y = frequency)) +\n  geom_point() + \n  scale_y_continuous(limits = c(0, 0.7), breaks = c(seq(0, 0.7, 0.1))) +\n  xlab(NULL) + \n  ylab(\"Relative frequency\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\ndfm_weight_pres &lt;- data_corpus_inaugural %&gt;%\n  corpus_subset(Year &gt; 2000) %&gt;%\n  tokens(remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm() %&gt;%\n  dfm_weight(scheme = \"prop\")\n\n# Calculate relative frequency by president\nfreq_weight &lt;- textstat_frequency(dfm_weight_pres, n = 10, \n                                  groups = dfm_weight_pres$President)\n\nggplot(data = freq_weight, aes(x = nrow(freq_weight):1, y = frequency)) +\n  geom_point() +\n  facet_wrap(~ group, scales = \"free\") +\n  coord_flip() +\n  scale_x_continuous(breaks = nrow(freq_weight):1,\n                     labels = freq_weight$feature) +\n  labs(x = NULL, y = \"Relative frequency\")\n\n\n\n# Only select speeches by Obama and Trump\npres_corpus &lt;- corpus_subset(data_corpus_inaugural, \n                             President %in% c(\"Obama\", \"Trump\"))\n\n# Create a dfm grouped by president\npres_dfm &lt;- tokens(pres_corpus, remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  tokens_group(groups = President) %&gt;%\n  dfm()\n\n# Calculate keyness and determine Trump as target group\nresult_keyness &lt;- textstat_keyness(pres_dfm, target = \"Trump\")\n\n# Plot estimated word keyness\ntextplot_keyness(result_keyness) \n\n\n\n# Plot without the reference text (in this case Obama)\ntextplot_keyness(result_keyness, show_reference = FALSE)\n\n\n\nlibrary(quanteda.textmodels)\n\n# Irish budget speeches from 2010 (data from quanteda.textmodels)\n# Transform corpus to dfm\ndata(data_corpus_irishbudget2010, package = \"quanteda.textmodels\")\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Set reference scores\nrefscores &lt;- c(rep(NA, 4), 1, -1, rep(NA, 8))\n\n# Predict Wordscores model\nws &lt;- textmodel_wordscores(ie_dfm, y = refscores, smooth = 1)\n\n# Plot estimated word positions (highlight words and print them in red)\ntextplot_scale1d(ws,\n                 highlighted = c(\"minister\", \"have\", \"our\", \"budget\"), \n                 highlighted_color = \"red\")\n\n\n\n# Get predictions\npred &lt;- predict(ws, se.fit = TRUE)\n\n# Plot estimated document positions and group by \"party\" variable\ntextplot_scale1d(pred, margin = \"documents\",\n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n# Plot estimated document positions using the LBG transformation and group by \"party\" variable\n\npred_lbg &lt;- predict(ws, se.fit = TRUE, rescaling = \"lbg\")\n\ntextplot_scale1d(pred_lbg, margin = \"documents\",\n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n# Estimate Wordfish model\nlibrary(\"quanteda.textmodels\")\nwf &lt;- textmodel_wordfish(dfm(tokens(data_corpus_irishbudget2010)), dir = c(6, 5))\n\n# Plot estimated word positions\ntextplot_scale1d(wf, margin = \"features\", \n                 highlighted = c(\"government\", \"global\", \"children\", \n                                 \"bank\", \"economy\", \"the\", \"citizenship\",\n                                 \"productivity\", \"deficit\"), \n                 highlighted_color = \"red\")\n\n\n\n# Plot estimated document positions\ntextplot_scale1d(wf, groups = data_corpus_irishbudget2010$party)\n\n\n\n# Transform corpus to dfm\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Run correspondence analysis on dfm\nca &lt;- textmodel_ca(ie_dfm)\n\n# Plot estimated positions and group by party\ntextplot_scale1d(ca, margin = \"documents\",\n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\n\nThe lexical dispersion plot reveals shifts in U.S. presidential rhetoric from Eisenhower to Biden, with the terms “American” and “people” appearing consistently but with varying emphasis. Presidents like Reagan, Clinton, Obama, and Trump frequently used both terms, reflecting a focus on national identity and populist themes. More recent presidents, especially Biden, show increased usage of “people,” suggesting a shift toward inclusive language. Earlier presidencies like Eisenhower and Johnson have fewer instances, indicating a leaner use of direct appeals to citizens in speeches, potentially due to changes in cultural and rhetorical trends over time.\n\nQ4. Wordfish was created for text data analysis, and it is particularly helpful for determining where documents fall on a latent scale based on word usage. By quantifying variations in word frequency among documents using a Poisson-based method, it enables researchers to deduce the ideological or policy stances of texts without being aware of those stances beforehand. Wordfish can be used to estimate these locations in policy documents, political speeches, or any corpus where positional scaling is important. Wordfish is useful for social science and political science research because it can reveal hidden themes or points of view in texts by spotting unique word patterns.\nAssignment 4 (Data Collection)\nQ1.\n\n## Workshop: Scraping webpages with R rvest package\n# Prerequisites: Chrome browser, Selector Gadget\n\n# install.packages(\"tidyverse\")\nlibrary(tidyverse)\ninstall.packages(\"rvest\")\n\n\nThe downloaded binary packages are in\n    /var/folders/fp/9t0t7f910m9fbvmzj10jn0h80000gn/T//RtmpX9k5lZ/downloaded_packages\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(readr)\n\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'\n#Reading the HTML code from the Wiki website\nwikiforreserve &lt;- read_html(url)\nclass(wikiforreserve)\n\n[1] \"xml_document\" \"xml_node\"    \n\n## Get the XPath data using Inspect element feature in Safari, Chrome or Firefox\n## At Inspect tab, look for &lt;table class=....&gt; tag. Leave the table close\n## Right click the table and Copy --&gt; XPath, paste at html_nodes(xpath =)\n\nforeignreserve &lt;- wikiforreserve %&gt;%\n  html_nodes(xpath='//*[@id=\"mw-content-text\"]/div[1]/table[1]') %&gt;%\n  html_table()\nclass(foreignreserve) \n\n[1] \"list\"\n\n# Why the first column is not scrapped?\n# Answer: This could happen due to table headers or inconsistent formatting in the HTML structure, \n# or if the first column is a row header or merged cell, which may not be captured in the table structure as expected by html_table.\n\nfores = foreignreserve[[1]][,c(1, 2,3,4,5,6,7,8) ] # [[ ]] returns a single element directly, without retaining the list structure.\n\n\n# \nnames(fores) &lt;- c(\"Country\", \"Forexreswithgold\", \"Date1\", \"Change1\",\"Forexreswithoutgold\", \"Date2\",\"Change2\", \"Sources\")\ncolnames(fores)\n\n[1] \"Country\"             \"Forexreswithgold\"    \"Date1\"              \n[4] \"Change1\"             \"Forexreswithoutgold\" \"Date2\"              \n[7] \"Change2\"             \"Sources\"            \n\nhead(fores$Country, n=10)\n\n [1] \"Country(as recognized by the U.N.)\" \"China\"                             \n [3] \"Japan\"                              \"Switzerland\"                       \n [5] \"India\"                              \"Russia\"                            \n [7] \"Taiwan\"                             \"Saudi Arabia\"                      \n [9] \"South Korea\"                        \"Hong Kong\"                         \n\n# Sources column useful?\n# Answer: The Column names are large so it could cause problem in code.That is the reson we changed these names. \n# Now I don't think they are useful\n\n## Clean up variables\n## What type is Date?\nstr(fores) # Dates are character in this case\n\ntibble [195 × 8] (S3: tbl_df/tbl/data.frame)\n $ Country            : chr [1:195] \"Country(as recognized by the U.N.)\" \"China\" \"Japan\" \"Switzerland\" ...\n $ Forexreswithgold   : chr [1:195] \"Continent\" \"Asia\" \"Asia\" \"Europe\" ...\n $ Date1              : chr [1:195] \"Sub-region\" \"East Asia\" \"East Asia\" \"Western Europe\" ...\n $ Change1            : chr [1:195] \"U.S.$ millions\" \"3,571,803\" \"1,238,950\" \"952,687\" ...\n $ Forexreswithoutgold: chr [1:195] \"Last reported date\" \"31 Oct 2024\" \"1 Nov 2024\" \"30 Sep 2024\" ...\n $ Date2              : chr [1:195] \"Change\" \"21,957\" \"15,948\" \"1,127\" ...\n $ Change2            : chr [1:195] \"U.S.$ millions\" \"3,380,334\" \"1,164,583\" \"864,519\" ...\n $ Sources            : chr [1:195] \"Last reported date\" \"31 Oct 2024\" \"1 Nov 2024\" \"30 Sep 2024\" ...\n\n# Convert Date1 variable\nfores$Date1 = as.Date(fores$Date1, format = \"%d %b %Y\")\nclass(fores$Date1)\n\n[1] \"Date\"\n\nwrite.csv(fores, \"fores.csv\", row.names = FALSE) \n \n# use fwrite?\ninstall.packages(\"data.table\")\n\n\nThe downloaded binary packages are in\n    /var/folders/fp/9t0t7f910m9fbvmzj10jn0h80000gn/T//RtmpX9k5lZ/downloaded_packages\n\n# Load the data.table package\nlibrary(data.table)\n\nWarning: package 'data.table' was built under R version 4.3.3\n\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nfwrite(fores, \"fores.csv\",row.names = FALSE)\n\n################################################## New Table #####################################\n\n# Prerequisites: Chrome browser, Selector Gadget\n\n# install.packages(\"tidyverse\")\nlibrary(tidyverse)\ninstall.packages(\"rvest\")\n\n\nThe downloaded binary packages are in\n    /var/folders/fp/9t0t7f910m9fbvmzj10jn0h80000gn/T//RtmpX9k5lZ/downloaded_packages\n\nlibrary(rvest)\nlibrary(readr)\n\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'\n#Reading the HTML code from the Wiki website\nwikiforreserve &lt;- read_html(url)\nclass(wikiforreserve)\n\n[1] \"xml_document\" \"xml_node\"    \n\n## Get the XPath data using Inspect element feature in Safari, Chrome or Firefox\n## At Inspect tab, look for &lt;table class=....&gt; tag. Leave the table close\n## Right click the table and Copy --&gt; XPath, paste at html_nodes(xpath =)\n\ncurrency &lt;- wikiforreserve %&gt;%\n  html_nodes(xpath='//*[@id=\"mw-content-text\"]/div[1]/table[2]') %&gt;%\n  html_table()\nclass(foreignreserve) \n\n[1] \"list\"\n\n# Why the first column is not scrapped?\n# Answer: This could happen due to table headers or inconsistent formatting in the HTML structure, \n# or if the first column is a row header or merged cell, which may not be captured in the table structure as expected by html_table.\n\nfores = currency[[1]][,c(2,3,4,5,6,7,8) ] # [[ ]] returns a single element directly, without retaining the list structure.\n\n# \nnames(fores) &lt;- c(\"Year\", \"Quater\", \"USD\", \"EUR\",\"JPY\", \"GBP\",\"CAD\")\ncolnames(fores)\n\n[1] \"Year\"   \"Quater\" \"USD\"    \"EUR\"    \"JPY\"    \"GBP\"    \"CAD\"   \n\nhead(fores, n=10)\n\n# A tibble: 10 × 7\n    Year Quater USD      EUR      JPY    GBP    CAD   \n   &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; \n 1    NA \"\"     USD      EUR      JPY    GBP    CAD   \n 2  2019 \"Q1\"   6,727.09 2,208.79 584.63 495.70 208.64\n 3  2019 \"Q2\"   6,752.28 2,264.88 611.87 497.41 209.85\n 4  2019 \"Q3\"   6,728.85 2,212.74 612.75 492.22 205.44\n 5  2019 \"Q4\"   6,674.83 2,279.30 631.00 511.51 206.71\n 6  2020 \"Q1\"   6,794.91 2,197.30 624.97 486.08 195.13\n 7  2020 \"Q2\"   6,902.01 2,272.44 643.70 504.36 215.47\n 8  2020 \"Q3\"   6,927.16 2,359.61 668.19 523.64 231.10\n 9  2020 \"Q4\"   6,990.97 2,526.41 715.35 561.39 246.57\n10  2021 \"Q1\"   6,971.79 2,404.80 686.30 554.28 250.01\n\nfores &lt;- fores[-c(1), ]\n\n## Clean up variables\n## What type is Date?\nstr(fores)\n\ntibble [24 × 7] (S3: tbl_df/tbl/data.frame)\n $ Year  : int [1:24] 2019 2019 2019 2019 2020 2020 2020 2020 2021 2021 ...\n $ Quater: chr [1:24] \"Q1\" \"Q2\" \"Q3\" \"Q4\" ...\n $ USD   : chr [1:24] \"6,727.09\" \"6,752.28\" \"6,728.85\" \"6,674.83\" ...\n $ EUR   : chr [1:24] \"2,208.79\" \"2,264.88\" \"2,212.74\" \"2,279.30\" ...\n $ JPY   : chr [1:24] \"584.63\" \"611.87\" \"612.75\" \"631.00\" ...\n $ GBP   : chr [1:24] \"495.70\" \"497.41\" \"492.22\" \"511.51\" ...\n $ CAD   : chr [1:24] \"208.64\" \"209.85\" \"205.44\" \"206.71\" ...\n\n# use fwrite?\ninstall.packages(\"data.table\")\n\n\nThe downloaded binary packages are in\n    /var/folders/fp/9t0t7f910m9fbvmzj10jn0h80000gn/T//RtmpX9k5lZ/downloaded_packages\n\n# Load the data.table package\nlibrary(data.table)\n\nfwrite(fores, \"currency.csv\",row.names = FALSE)\n\nQ2.\nAssignment 5 (Data Collection)"
  },
  {
    "objectID": "data_visualization.html",
    "href": "data_visualization.html",
    "title": "Vinay Kumar",
    "section": "",
    "text": "Pursuing MSc. in Quantitative Economics and Econometrics at UTD"
  },
  {
    "objectID": "data_collection.html",
    "href": "data_collection.html",
    "title": "Vinay Kumar",
    "section": "",
    "text": "Pursuing MSc. in Quantitative Economics and Econometrics at UTD"
  }
]